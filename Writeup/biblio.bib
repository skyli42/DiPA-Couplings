@inproceedings{abowdCensusBureauAdopts2018a,
  title = {The {{U}}.{{S}}. {{Census Bureau Adopts Differential Privacy}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Abowd, John M.},
  year = {2018},
  month = jul,
  series = {{{KDD}} '18},
  pages = {2867},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3219819.3226070},
  urldate = {2024-03-19},
  abstract = {The U.S. Census Bureau announced, via its Scientific Advisory Committee, that it would protect the publications of the 2018 End-to-End Census Test (E2E) using differential privacy. The E2E test is a dress rehearsal for the 2020 Census, the constitutionally mandated enumeration of the population used to reapportion the House of Representatives and redraw every legislative district in the country. Systems that perform successfully in the E2E test are then used in the production of the 2020 Census. Motivation: The Census Bureau conducted internal research that confirmed that the statistical disclosure limitation systems used for the 2000 and 2010 Censuses had serious vulnerabilities that were exposed by the Dinur and Nissim (2003) database reconstruction theorem. We designed a differentially private publication system that directly addressed these vulnerabilities while preserving the fitness for use of the core statistical products. Problem statement: Designing and engineering production differential privacy systems requires two primary components: (1) inventing and constructing algorithms that deliver maximum accuracy for a given privacy-loss budget and (2) insuring that the privacy-loss budget can be directly controlled by the policy-makers who must choose an appropriate point on the accuracy-privacy-loss tradeoff. The first problem lies in the domain of computer science. The second lies in the domain of economics. Approach: The algorithms under development for the 2020 Census focus on the data used to draw legislative districts and to enforce the 1965 Voting Rights Act (VRA). These algorithms efficiently distribute the noise injected by differential privacy. The Data Stewardship Executive Policy Committee selects the privacy-loss parameter after reviewing accuracy-privacy-loss graphs.},
  isbn = {978-1-4503-5552-0},
  keywords = {differential privacy,economics of privacy},
  file = {C:\Users\skyli\Zotero\storage\3U4NUPWZ\Abowd - 2018 - The U.S. Census Bureau Adopts Differential Privacy.pdf}
}

@inproceedings{afoninMinimalUnionFreeDecompositions2009,
  title = {Minimal {{Union-Free Decompositions}} of {{Regular Languages}}},
  booktitle = {Language and {{Automata Theory}} and {{Applications}}},
  author = {Afonin, Sergey and Golomazov, Denis},
  editor = {Dediu, Adrian Horia and Ionescu, Armand Mihai and {Mart{\'i}n-Vide}, Carlos},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {83--92},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-00982-2_7},
  abstract = {A regular language is called union-free if it can be represented by a regular expression that does not contain the union operation. Every regular language can be represented as a finite union of union-free languages (the so-called union-free decomposition), but such decomposition is not necessarily unique. We call the number of components in the minimal union-free decomposition of a regular language the union width of the regular language. In this paper we prove that the union width of any regular language can be effectively computed and we present an algorithm for constructing a corresponding decomposition. We also study some properties of union-free languages and introduce a new algorithm for checking whether a regular language is union-free.},
  isbn = {978-3-642-00982-2},
  langid = {english},
  keywords = {Rational Subset,Regular Expression,Regular Language,Short Word,Union Operation},
  file = {C:\Users\skyli\Zotero\storage\T9QVEMLJ\Afonin and Golomazov - 2009 - Minimal Union-Free Decompositions of Regular Langu.pdf}
}

@inproceedings{albarghouthiConstraintBasedSynthesisCoupling2018,
  title = {Constraint-{{Based Synthesis}} of {{Coupling Proofs}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Albarghouthi, Aws and Hsu, Justin},
  editor = {Chockler, Hana and Weissenbacher, Georg},
  year = {2018},
  pages = {327--346},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-96145-3_18},
  abstract = {Proof by coupling is a classical technique for proving properties about pairs of randomized algorithms by carefully relating (or coupling) two probabilistic executions. In this paper, we show how to automatically construct such proofs for probabilistic programs. First, we present f-coupled postconditions, an abstraction describing two correlated program executions. Second, we show how properties of f-coupled postconditions can imply various probabilistic properties of the original programs. Third, we demonstrate how to reduce the proof-search problem to a purely logical synthesis problem of the form , making probabilistic reasoning unnecessary. We develop a prototype implementation to automatically build coupling proofs for probabilistic properties, including uniformity and independence of program expressions.},
  isbn = {978-3-319-96145-3},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\H98G8SF2\Albarghouthi and Hsu - 2018 - Constraint-Based Synthesis of Coupling Proofs.pdf}
}

@article{albarghouthiSynthesizingCouplingProofs2017,
  title = {Synthesizing Coupling Proofs of Differential Privacy},
  author = {Albarghouthi, Aws and Hsu, Justin},
  year = {2017},
  month = dec,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {POPL},
  pages = {58:1--58:30},
  doi = {10.1145/3158146},
  urldate = {2024-03-19},
  abstract = {Differential privacy has emerged as a promising probabilistic formulation of privacy, generating intense interest within academia and industry. We present a push-button, automated technique for verifying {$\varepsilon$}-differential privacy of sophisticated randomized algorithms. We make several conceptual, algorithmic, and practical contributions: (i) Inspired by the recent advances on approximate couplings and randomness alignment, we present a new proof technique called coupling strategies, which casts differential privacy proofs as a winning strategy in a game where we have finite privacy resources to expend. (ii) To discover a winning strategy, we present a constraint-based formulation of the problem as a set of Horn modulo couplings (HMC) constraints, a novel combination of first-order Horn clauses and probabilistic constraints. (iii) We present a technique for solving HMC constraints by transforming probabilistic constraints into logical constraints with uninterpreted functions. (iv) Finally, we implement our technique in the FairSquare verifier and provide the first automated privacy proofs for a number of challenging algorithms from the differential privacy literature, including Report Noisy Max, the Exponential Mechanism, and the Sparse Vector Mechanism.},
  keywords = {Differential Privacy,Synthesis},
  file = {C:\Users\skyli\Zotero\storage\VP7TNG2V\Albarghouthi and Hsu - 2017 - Synthesizing coupling proofs of differential priva.pdf}
}

@article{bartheDecidingAccuracyDifferential2021,
  title = {Deciding Accuracy of Differential Privacy Schemes},
  author = {Barthe, Gilles and Chadha, Rohit and Krogmeier, Paul and Sistla, A. Prasad and Viswanathan, Mahesh},
  year = {2021},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {5},
  number = {POPL},
  pages = {8:1--8:30},
  doi = {10.1145/3434289},
  urldate = {2024-03-19},
  abstract = {Differential privacy is a mathematical framework for developing statistical computations with provable guarantees of privacy and accuracy. In contrast to the privacy component of differential privacy, which has a clear mathematical and intuitive meaning, the accuracy component of differential privacy does not have a generally accepted definition; accuracy claims of differential privacy algorithms vary from algorithm to algorithm and are not instantiations of a general definition. We identify program discontinuity as a common theme in existing ad hoc definitions and introduce an alternative notion of accuracy parametrized by, what we call, --- the of an input x w.r.t. a deterministic computation f and a distance d, is the minimal distance d(x,y) over all y such that f(y){$\neq$} f(x). We show that our notion of accuracy subsumes the definition used in theoretical computer science, and captures known accuracy claims for differential privacy algorithms. In fact, our general notion of accuracy helps us prove better claims in some cases. Next, we study the decidability of accuracy. We first show that accuracy is in general undecidable. Then, we define a non-trivial class of probabilistic computations for which accuracy is decidable (unconditionally, or assuming Schanuel's conjecture). We implement our decision procedure and experimentally evaluate the effectiveness of our approach for generating proofs or counterexamples of accuracy for common algorithms from the literature.},
  keywords = {accuracy,decidability,differential privacy},
  file = {C:\Users\skyli\Zotero\storage\S447RLLE\Barthe et al. - 2021 - Deciding accuracy of differential privacy schemes.pdf}
}

@inproceedings{bartheDecidingDifferentialPrivacy2020,
  title = {Deciding {{Differential Privacy}} for {{Programs}} with {{Finite Inputs}} and {{Outputs}}},
  booktitle = {Proceedings of the 35th {{Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Barthe, Gilles and Chadha, Rohit and Jagannath, Vishal and Sistla, A. Prasad and Viswanathan, Mahesh},
  year = {2020},
  month = jul,
  series = {{{LICS}} '20},
  pages = {141--154},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3373718.3394796},
  urldate = {2024-03-19},
  abstract = {Differential privacy is a de facto standard for statistical computations over databases that contain private data. Its main and rather surprising strength is to guarantee individual privacy and yet allow for accurate statistical results. Thanks to its mathematical definition, differential privacy is also a natural target for formal analysis. A broad line of work develops and uses logical methods for proving privacy. A more recent and complementary line of work uses statistical methods for finding privacy violations. Although both lines of work are practically successful, they elide the fundamental question of decidability. This paper studies the decidability of differential privacy. We first establish that checking differential privacy is undecidable even if one restricts to programs having a single Boolean input and a single Boolean output. Then, we define a non-trivial class of programs and provide a decision procedure for checking the differential privacy of a program in this class. Our procedure takes as input a program P parametrized by a privacy budget {$\epsilon$} and either establishes the differential privacy for all possible values of {$\epsilon$} or generates a counter-example. In addition, our procedure works for both to {$\epsilon$}-differential privacy and ({$\epsilon$}, {$\delta$})-differential privacy. Technically, the decision procedure is based on a novel and judicious encoding of the semantics of programs in our class into a decidable fragment of the first-order theory of the reals with exponentiation. We implement our procedure and use it for (dis)proving privacy bounds for many well-known examples, including randomized response, histogram, report noisy max and sparse vector.},
  isbn = {978-1-4503-7104-9},
  keywords = {decision procedure,differential privacy,sparse vector},
  file = {C:\Users\skyli\Zotero\storage\QKZNVV9P\Barthe et al. - 2020 - Deciding Differential Privacy for Programs with Fi.pdf}
}

@incollection{bartheDifferentialPrivacyComposition2013,
  title = {Beyond {{Differential Privacy}}: {{Composition Theorems}} and {{Relational Logic}} for f-Divergences between {{Probabilistic Programs}}},
  shorttitle = {Beyond {{Differential Privacy}}},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  author = {Barthe, Gilles and Olmedo, Federico},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fomin, Fedor V. and Freivalds, R{\=u}si{\c n}{\v s} and Kwiatkowska, Marta and Peleg, David},
  year = {2013},
  volume = {7966},
  pages = {49--60},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-39212-2_8},
  urldate = {2024-03-19},
  isbn = {978-3-642-39211-5 978-3-642-39212-2},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\ZWMK2SNN\Barthe and Olmedo - 2013 - Beyond Differential Privacy Composition Theorems .pdf}
}

@article{bartheProbabilisticRelationalReasoning2013,
  title = {Probabilistic {{Relational Reasoning}} for {{Differential Privacy}}},
  author = {Barthe, Gilles and K{\"o}pf, Boris and Olmedo, Federico and {Zanella-B{\'e}guelin}, Santiago},
  year = {2013},
  month = nov,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {35},
  number = {3},
  pages = {9:1--9:49},
  issn = {0164-0925},
  doi = {10.1145/2492061},
  urldate = {2024-03-19},
  abstract = {Differential privacy is a notion of confidentiality that allows useful computations on sensible data while protecting the privacy of individuals. Proving differential privacy is a difficult and error-prone task that calls for principled approaches and tool support. Approaches based on linear types and static analysis have recently emerged; however, an increasing number of programs achieve privacy using techniques that fall out of their scope. Examples include programs that aim for weaker, approximate differential privacy guarantees and programs that achieve differential privacy without using any standard mechanisms. Providing support for reasoning about the privacy of such programs has been an open problem. We report on CertiPriv, a machine-checked framework for reasoning about differential privacy built on top of the Coq proof assistant. The central component of CertiPriv is a quantitative extension of probabilistic relational Hoare logic that enables one to derive differential privacy guarantees for programs from first principles. We demonstrate the applicability of CertiPriv on a number of examples whose formal analysis is out of the reach of previous techniques. In particular, we provide the first machine-checked proofs of correctness of the Laplacian, Gaussian, and exponential mechanisms and of the privacy of randomized and streaming algorithms from the literature.},
  keywords = {Coq proof assistant,differential privacy,relational Hoare logic},
  file = {C:\Users\skyli\Zotero\storage\YGKN6WAH\Barthe et al. - 2013 - Probabilistic Relational Reasoning for Differentia.pdf}
}

@article{bartheProgrammingLanguageTechniques2016,
  title = {Programming Language Techniques for Differential Privacy},
  author = {Barthe, Gilles and Gaboardi, Marco and Hsu, Justin and Pierce, Benjamin},
  year = {2016},
  month = feb,
  journal = {ACM SIGLOG News},
  volume = {3},
  number = {1},
  pages = {34--53},
  doi = {10.1145/2893582.2893591},
  urldate = {2024-03-20},
  abstract = {Differential privacy is rigorous framework for stating and enforcing privacy guarantees on computations over sensitive data. Informally, differential privacy ensures that the presence or absence of a single individual in a database has only a negligible statistical effect on the computation's result. Many specific algorithms have been proved differentially private, but manually checking that a given program is differentially private can be subtle, tedious, or both. This approach becomes unfeasible when larger programs are considered.}
}

@inproceedings{bartheProvingDifferentialPrivacy2016,
  title = {Proving {{Differential Privacy}} via {{Probabilistic Couplings}}},
  booktitle = {Proceedings of the 31st {{Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Barthe, Gilles and Gaboardi, Marco and Gr{\'e}goire, Benjamin and Hsu, Justin and Strub, Pierre-Yves},
  year = {2016},
  month = jul,
  series = {{{LICS}} '16},
  pages = {749--758},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2933575.2934554},
  urldate = {2024-03-19},
  abstract = {Over the last decade, differential privacy has achieved widespread adoption within the privacy community. Moreover, it has attracted significant attention from the verification community, resulting in several successful tools for formally proving differential privacy. Although their technical approaches vary greatly, all existing tools rely on reasoning principles derived from the composition theorem of differential privacy. While this suffices to verify most common private algorithms, there are several important algorithms whose privacy analysis does not rely solely on the composition theorem. Their proofs are significantly more complex, and are currently beyond the reach of verification tools. In this paper, we develop compositional methods for formally verifying differential privacy for algorithms whose analysis goes beyond the composition theorem. Our methods are based on deep connections between differential privacy and probabilistic couplings, an established mathematical tool for reasoning about stochastic processes. Even when the composition theorem is not helpful, we can often prove privacy by a coupling argument. We demonstrate our methods on two algorithms: the Exponential mechanism and the Above Threshold algorithm, the critical component of the famous Sparse Vector algorithm. We verify these examples in a relational program logic apRHL+, which can construct approximate couplings. This logic extends the existing apRHL logic with more general rules for the Laplace mechanism and the one-sided Laplace mechanism, and new structural rules enabling pointwise reasoning about privacy; all the rules are inspired by the connection with coupling. While our paper is presented from a formal verification perspective, we believe that its main insight is of independent interest for the differential privacy community.},
  isbn = {978-1-4503-4391-6},
  file = {C:\Users\skyli\Zotero\storage\2HHQ2TDW\Barthe et al. - 2016 - Proving Differential Privacy via Probabilistic Cou.pdf}
}

@inproceedings{bartheRelationalReasoningProbabilistic2015a,
  title = {Relational {{Reasoning}} via {{Probabilistic Coupling}}},
  booktitle = {Logic for {{Programming}}, {{Artificial Intelligence}}, and {{Reasoning}}},
  author = {Barthe, Gilles and Espitau, Thomas and Gr{\'e}goire, Benjamin and Hsu, Justin and Stefanesco, L{\'e}o and Strub, Pierre-Yves},
  editor = {Davis, Martin and Fehnker, Ansgar and McIver, Annabelle and Voronkov, Andrei},
  year = {2015},
  pages = {387--401},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-48899-7_27},
  abstract = {Probabilistic coupling is a powerful tool for analyzing pairs of probabilistic processes. Roughly, coupling two processes requires finding an appropriate witness process that models both processes in the same probability space. Couplings are powerful tools proving properties about the relation between two processes, include reasoning about convergence of distributions and stochastic dominance---a probabilistic version of a monotonicity property.},
  isbn = {978-3-662-48899-7},
  langid = {english},
  keywords = {Fair Coin,Loop Body,Program Verification,Proof Assistant,Stochastic Dominance},
  file = {C:\Users\skyli\Zotero\storage\TB46SZNP\Barthe et al. - 2015 - Relational Reasoning via Probabilistic Coupling.pdf}
}

@article{bartheRelationalStarLiftings2019,
  title = {Relational \${\textbackslash}star\$-{{Liftings}} for {{Differential Privacy}}},
  author = {Barthe, Gilles and Espitau, Thomas and Hsu, Justin and Sato, Tetsuya and Strub, Pierre-Yves},
  year = {2019},
  month = dec,
  journal = {Logical Methods in Computer Science},
  volume = {Volume 15, Issue 4},
  publisher = {Episciences.org},
  issn = {1860-5974},
  doi = {10.23638/LMCS-15(4:18)2019},
  urldate = {2024-03-20},
  abstract = {Recent developments in formal verification have identified approximate liftings (also known as approximate couplings) as a clean, compositional abstraction for proving differential privacy. This construction can be defined in two styles. Earlier definitions require the existence of one or more witness distributions, while a recent definition by Sato uses universal quantification over all sets of samples. These notions have each have their own strengths: the universal version is more general than the existential ones, while existential liftings are known to satisfy more precise composition principles. We propose a novel, existential version of approximate lifting, called \${\textbackslash}star\$-lifting, and show that it is equivalent to Sato's construction for discrete probability measures. Our work unifies all known notions of approximate lifting, yielding cleaner properties, more general constructions, and more precise composition theorems for both styles of lifting, enabling richer proofs of differential privacy. We also clarify the relation between existing definitions of approximate lifting, and consider more general approximate liftings based on \$f\$-divergences.},
  file = {C:\Users\skyli\Zotero\storage\3GBDFESX\Barthe et al. - 2019 - Relational $star$-Liftings for Differential Priva.pdf}
}

@inproceedings{bichselDPFinderFindingDifferential2018,
  title = {{{DP-Finder}}: {{Finding Differential Privacy Violations}} by {{Sampling}} and {{Optimization}}},
  shorttitle = {{{DP-Finder}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Bichsel, Benjamin and Gehr, Timon and {Drachsler-Cohen}, Dana and Tsankov, Petar and Vechev, Martin},
  year = {2018},
  month = oct,
  series = {{{CCS}} '18},
  pages = {508--524},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3243734.3243863},
  urldate = {2024-03-19},
  abstract = {We present DP-Finder, a novel approach and system that automatically derives lower bounds on the differential privacy enforced by algorithms. Lower bounds are practically useful as they can show tightness of existing upper bounds or even identify incorrect upper bounds. Computing a lower bound involves searching for a counterexample, defined by two neighboring inputs and a set of outputs, that identifies a large privacy violation. This is an inherently hard problem as finding such a counterexample involves inspecting a large (usually infinite) and sparse search space. To address this challenge, DP-Finder relies on two key insights. First, we introduce an effective and precise correlated sampling method to estimate the privacy violation of a counterexample. Second, we show how to obtain a differentiable version of the problem, enabling us to phrase the search task as an optimization objective to be maximized with state-of-the-art numerical optimizers. This allows us to systematically search for large privacy violations. Our experimental results indicate that DP-Finder is effective in computing differential privacy lower bounds for a number of randomized algorithms. For instance, it finds tight lower bounds in algorithms that obfuscate their input in a non-trivial fashion.},
  isbn = {978-1-4503-5693-0},
  keywords = {differential privacy,lower bounds,optimization,sampling}
}

@inproceedings{bichselDPSniperBlackBoxDiscovery2021,
  title = {{{DP-Sniper}}: {{Black-Box Discovery}} of {{Differential Privacy Violations}} Using {{Classifiers}}},
  shorttitle = {{{DP-Sniper}}},
  booktitle = {2021 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Bichsel, Benjamin and Steffen, Samuel and Bogunovic, Ilija and Vechev, Martin},
  year = {2021},
  month = may,
  pages = {391--409},
  issn = {2375-1207},
  doi = {10.1109/SP40001.2021.00081},
  urldate = {2024-03-19},
  abstract = {We present DP-Sniper, a practical black-box method that automatically finds violations of differential privacy.DP-Sniper is based on two key ideas: (i) training a classifier to predict if an observed output was likely generated from one of two possible inputs, and (ii) transforming this classifier into an approximately optimal attack on differential privacy.Our experimental evaluation demonstrates that DP-Sniper obtains up to 12.4 times stronger guarantees than state-of-the-art, while being 15.5 times faster. Further, we show that DP-Sniper is effective in exploiting floating-point vulnerabilities of naively implemented algorithms: it detects that a supposedly 0.1-differentially private implementation of the Laplace mechanism actually does not satisfy even 0.25-differential privacy.},
  keywords = {Approximation algorithms,Classification algorithms,classifiers,differential distinguishability,differential privacy,Differential privacy,inference attacks,machine learning,Privacy,Security,Training},
  file = {C:\Users\skyli\Zotero\storage\MTNRQ66M\Bichsel et al. - 2021 - DP-Sniper Black-Box Discovery of Differential Pri.pdf}
}

@misc{brzozowskiMostComplexDeterministic2018,
  title = {Most {{Complex Deterministic Union-Free Regular Languages}}},
  author = {Brzozowski, Janusz A. and Davies, Sylvie},
  year = {2018},
  month = jan,
  number = {arXiv:1711.09149},
  eprint = {1711.09149},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.09149},
  urldate = {2024-02-16},
  abstract = {A regular language \$L\$ is union-free if it can be represented by a regular expression without the union operation. A union-free language is deterministic if it can be accepted by a deterministic one-cycle-free-path finite automaton; this is an automaton which has one final state and exactly one cycle-free path from any state to the final state. Jir{\textbackslash}'askov{\textbackslash}'a and Masopust proved that the state complexities of the basic operations reversal, star, product, and boolean operations in deterministic union-free languages are exactly the same as those in the class of all regular languages. To prove that the bounds are met they used five types of automata, involving eight types of transformations of the set of states of the automata. We show that for each \$n{\textbackslash}ge 3\$ there exists one ternary witness of state complexity \$n\$ that meets the bound for reversal and product. Moreover, the restrictions of this witness to binary alphabets meet the bounds for star and boolean operations. We also show that the tight upper bounds on the state complexity of binary operations that take arguments over different alphabets are the same as those for arbitrary regular languages. Furthermore, we prove that the maximal syntactic semigroup of a union-free language has \$n\^{}n\$ elements, as in the case of regular languages, and that the maximal state complexities of atoms of union-free languages are the same as those for regular languages. Finally, we prove that there exists a most complex union-free language that meets the bounds for all these complexity measures. Altogether this proves that the complexity measures above cannot distinguish union-free languages from regular languages.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Formal Languages and Automata Theory},
  file = {C\:\\Users\\skyli\\Zotero\\storage\\W8BPLM5X\\Brzozowski and Davies - 2018 - Most Complex Deterministic Union-Free Regular Lang.pdf;C\:\\Users\\skyli\\Zotero\\storage\\BF4AL28D\\1711.html}
}

@inproceedings{bunComplexityVerifyingBoolean2022,
  title = {The {{Complexity}} of {{Verifying Boolean Programs}} as {{Differentially Private}}},
  booktitle = {2022 {{IEEE}} 35th {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Bun, Mark and Gaboardi, Marco and Glinskih, Ludmila},
  year = {2022},
  month = aug,
  pages = {396--411},
  issn = {1940-1434},
  doi = {10.1109/CSF54842.2022.9919653},
  urldate = {2024-03-19},
  abstract = {We study the complexity of the problem of verifying differential privacy for while-like programs working over boolean values and making probabilistic choices. Programs in this class can be interpreted into finite-state discrete-time Markov Chains (DTMC). We show that the problem of deciding whether a program is differentially private for specific values of the privacy parameters is PSPACE-complete. To show that this problem is in PSPACE, we adapt classical results about computing hitting probabilities for DTMC. To show PSPACE-hardness we use a reduction from the problem of checking whether a program almost surely terminates or not. We also show that the problem of approximating the privacy parameters that a program provides is PSPACE-hard. Moreover, we investigate the complexity of similar problems also for several relaxations of differential privacy: Renyi differential privacy, concentrated differential privacy, and truncated concentrated differential privacy. For these notions, we consider gap-versions of the problem of deciding whether a program is private or not and we show that all of them are PSPACE-complete.},
  keywords = {Complexity theory,Computer security,differential privacy,Differential privacy,Markov processes,Privacy,Probabilistic logic,probabilistic programs,while programs},
  file = {C:\Users\skyli\Zotero\storage\JYAAGJ3I\Bun et al. - 2022 - The Complexity of Verifying Boolean Programs as Di.pdf}
}

@inproceedings{bunComplexityVerifyingBoolean2022a,
  title = {The {{Complexity}} of {{Verifying Boolean Programs}} as {{Differentially Private}}},
  booktitle = {2022 {{IEEE}} 35th {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Bun, Mark and Gaboardi, Marco and Glinskih, Ludmila},
  year = {2022},
  month = aug,
  pages = {396--411},
  publisher = {IEEE Computer Society},
  issn = {1940-1434},
  doi = {10.1109/CSF54842.2022.9919653},
  urldate = {2024-03-20},
  abstract = {We study the complexity of the problem of verifying differential privacy for while-like programs working over boolean values and making probabilistic choices. Programs in this class can be interpreted into finite-state discrete-time Markov Chains (DTMC). We show that the problem of deciding whether a program is differentially private for specific values of the privacy parameters is PSPACE-complete. To show that this problem is in PSPACE, we adapt classical results about computing hitting probabilities for DTMC. To show PSPACE-hardness we use a reduction from the problem of checking whether a program almost surely terminates or not. We also show that the problem of approximating the privacy parameters that a program provides is PSPACE-hard. Moreover, we investigate the complexity of similar problems also for several relaxations of differential privacy: Renyi differential privacy, concentrated differential privacy, and truncated concentrated differential privacy. For these notions, we consider gap-versions of the problem of deciding whether a program is private or not and we show that all of them are PSPACE-complete.},
  isbn = {978-1-66548-417-6},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\8BR4YSXN\Bun et al. - 2022 - The Complexity of Verifying Boolean Programs as Di.pdf}
}

@inproceedings{chadhaDecidingDifferentialPrivacy2023,
  title = {Deciding {{Differential Privacy}} of {{Online Algorithms}} with {{Multiple Variables}}},
  booktitle = {Proceedings of the 2023 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Chadha, Rohit and Sistla, A. Prasad and Viswanathan, Mahesh and Bhusal, Bishnu},
  year = {2023},
  month = nov,
  series = {{{CCS}} '23},
  pages = {1761--1775},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3576915.3623170},
  urldate = {2024-03-19},
  abstract = {We consider the problem of checking the differential privacy of online randomized algorithms that process a stream of inputs and produce outputs corresponding to each input. This paper generalizes an automaton model called DiP automata [10] to describe such algorithms by allowing multiple real-valued storage variables. A DiP automaton is a parametric automaton whose behavior depends on the privacy budget {$\in$}. An automaton A will be said to be differentially private if, for some D, the automaton is D{$\in$}-differentially private for all values of {$\in$} {$>$} 0. We identify a precise characterization of the class of all differentially private DiP automata. We show that the problem of determining if a given DiP automaton belongs to this class is PSPACE-complete. Our PSPACE algorithm also computes a value for D when the given automaton is differentially private. The algorithm has been implemented, and experiments demonstrating its effectiveness are presented.},
  isbn = {9798400700507},
  keywords = {automata,decision procedure,differential privacy,verification},
  file = {C:\Users\skyli\Zotero\storage\KB6BXMPU\Chadha et al. - 2023 - Deciding Differential Privacy of Online Algorithms.pdf}
}

@inproceedings{chadhaLinearTimeDecidability2021,
  title = {On Linear Time Decidability of Differential Privacy for Programs with Unbounded Inputs},
  booktitle = {Proceedings of the 36th {{Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Chadha, Rohit and Sistla, A. Prasad and Viswanathan, Mahesh},
  year = {2021},
  month = nov,
  series = {{{LICS}} '21},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1109/LICS52264.2021.9470708},
  urldate = {2024-03-19},
  abstract = {We introduce an automata model for describing interesting classes of differential privacy mechanisms/algorithms that include known mechanisms from the literature. These automata can model algorithms whose inputs can be an unbounded sequence of real-valued query answers. We consider the problem of checking whether there exists a constant d such that the algorithm described by these automata are d{$\in$}-differentially private for all positive values of the privacy budget parameter {$\in$}. We show that this problem can be decided in time linear in the automaton's size by identifying a necessary and sufficient condition on the underlying graph of the automaton. This paper's results are the first decidability results known for algorithms with an unbounded number of query answers taking values from the set of reals.},
  isbn = {978-1-66544-895-6},
  file = {C:\Users\skyli\Zotero\storage\IS2SY8RG\Chadha et al. - 2021 - On linear time decidability of differential privac.pdf}
}

@inproceedings{dingDetectingViolationsDifferential2018,
  title = {Detecting {{Violations}} of {{Differential Privacy}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Ding, Zeyu and Wang, Yuxin and Wang, Guanhong and Zhang, Danfeng and Kifer, Daniel},
  year = {2018},
  month = oct,
  series = {{{CCS}} '18},
  pages = {475--489},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3243734.3243818},
  urldate = {2024-03-19},
  abstract = {The widespread acceptance of differential privacy has led to the publication of many sophisticated algorithms for protecting privacy. However, due to the subtle nature of this privacy definition, many such algorithms have bugs that make them violate their claimed privacy. In this paper, we consider the problem of producing counterexamples for such incorrect algorithms. The counterexamples are designed to be short and human-understandable so that the counterexample generator can be used in the development process -- a developer could quickly explore variations of an algorithm and investigate where they break down. Our approach is statistical in nature. It runs a candidate algorithm many times and uses statistical tests to try to detect violations of differential privacy. An evaluation on a variety of incorrect published algorithms validates the usefulness of our approach: it correctly rejects incorrect algorithms and provides counterexamples for them within a few seconds.},
  isbn = {978-1-4503-5693-0},
  keywords = {counterexample detection,differential privacy,statistical testing},
  file = {C:\Users\skyli\Zotero\storage\VUKN9I7G\Ding et al. - 2018 - Detecting Violations of Differential Privacy.pdf}
}

@article{dworkAlgorithmicFoundationsDifferential2014b,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2014},
  month = aug,
  journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume = {9},
  number = {3--4},
  pages = {211--407},
  publisher = {Now Publishers, Inc.},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  urldate = {2024-03-19},
  abstract = {The Algorithmic Foundations of Differential Privacy},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\M24ZTKNV\Dwork and Roth - 2014 - The Algorithmic Foundations of Differential Privac.pdf}
}

@inproceedings{dworkCalibratingNoiseSensitivity2006a,
  title = {Calibrating {{Noise}} to {{Sensitivity}} in {{Private Data Analysis}}},
  booktitle = {Theory of {{Cryptography}}},
  author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  editor = {Halevi, Shai and Rabin, Tal},
  year = {2006},
  pages = {265--284},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11681878_14},
  abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
  isbn = {978-3-540-32732-5},
  langid = {english},
  keywords = {Laplace Distribution,Privacy Breach,Query Function,Semantic Security,True Answer},
  file = {C:\Users\skyli\Zotero\storage\U6W8DHKU\Dwork et al. - 2006 - Calibrating Noise to Sensitivity in Private Data A.pdf}
}

@inproceedings{farinaCoupledRelationalSymbolic2021,
  title = {Coupled {{Relational Symbolic Execution}} for {{Differential Privacy}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Farina, Gian Pietro and Chong, Stephen and Gaboardi, Marco},
  editor = {Yoshida, Nobuko},
  year = {2021},
  pages = {207--233},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-72019-3_8},
  abstract = {Differential privacy is a de facto standard in data privacy with applications in the private and public sectors. Most of the techniques that achieve differential privacy are based on a judicious use of randomness. However, reasoning about randomized programs is difficult and error prone. For this reason, several techniques have been recently proposed to support designer in proving programs differentially private or in finding violations to it.},
  isbn = {978-3-030-72019-3},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\8945J7BE\Farina et al. - 2021 - Coupled Relational Symbolic Execution for Differen.pdf}
}

@inproceedings{gaboardiComplexityVerifyingLoopFree2020,
  title = {The {{Complexity}} of {{Verifying Loop-Free Programs}} as {{Differentially Private}}},
  booktitle = {{{DROPS-IDN}}/v2/Document/10.4230/{{LIPIcs}}.{{ICALP}}.2020.129},
  author = {Gaboardi, Marco and Nissim, Kobbi and Purser, David},
  year = {2020},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  doi = {10.4230/LIPIcs.ICALP.2020.129},
  urldate = {2024-03-20},
  abstract = {We study the problem of verifying differential privacy for loop-free programs with probabilistic choice. Programs in this class can be seen as randomized Boolean circuits, which we will use as a formal model to answer two different questions: first, deciding whether a program satisfies a prescribed level of privacy; second, approximating the privacy parameters a program realizes. We show that the problem of deciding whether a program satisfies {$\varepsilon$}-differential privacy is coNP\^{}\#P-complete. In fact, this is the case when either the input domain or the output range of the program is large. Further, we show that deciding whether a program is ({$\varepsilon$},{$\delta$})-differentially private is coNP\^{}\#P-hard, and in coNP\^{}\#P for small output domains, but always in coNP\^{}\{\#P\^{}\#P\}. Finally, we show that the problem of approximating the level of differential privacy is both NP-hard and coNP-hard. These results complement previous results by Murtagh and Vadhan [Jack Murtagh and Salil P. Vadhan, 2016] showing that deciding the optimal composition of differentially private components is \#P-complete, and that approximating the optimal composition of differentially private components is in P.},
  copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\AW8ZHRTR\Gaboardi et al. - 2020 - The Complexity of Verifying Loop-Free Programs as .pdf}
}

@inproceedings{gadottiPoolInferenceAttacks2022,
  title = {Pool {{Inference Attacks}} on {{Local Differential Privacy}}: {{Quantifying}} the {{Privacy Guarantees}} of {{Apple}}'s {{Count Mean Sketch}} in {{Practice}}},
  shorttitle = {Pool {{Inference Attacks}} on {{Local Differential Privacy}}},
  booktitle = {31st {{USENIX Security Symposium}} ({{USENIX Security}} 22)},
  author = {Gadotti, Andrea and Houssiau, Florimond and Annamalai, Meenatchi Sundaram Muthu Selva and de Montjoye, Yves-Alexandre},
  year = {2022},
  pages = {501--518},
  urldate = {2023-12-10},
  isbn = {978-1-939133-31-1},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\IFECGGD2\Gadotti et al. - 2022 - Pool Inference Attacks on Local Differential Priva.pdf}
}

@misc{HowWeRe,
  title = {How We're Helping Developers with Differential Privacy},
  journal = {Google for Developers},
  urldate = {2024-03-19},
  abstract = {Explore the Google for Developers Blog which provides insights and the latest news about our AI, Cloud, Mobile and Web app development announcements.},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\AXE9G26A\how-were-helping-developers-with-differential-privacy.html},
  howpublished = {https://developers.googleblog.com/2021/01/how-were-helping-developers-with-differential-privacy.html},

}

@phdthesis{hsuProbabilisticCouplingsProbabilistic2017,
  title = {Probabilistic {{Couplings For Probabilistic Reasoning}}},
  author = {Hsu, Justin},
  year = {2017},
  month = jan,
  number = {3017},
  urldate = {2024-03-19},
  abstract = {This thesis explores proofs by coupling from the perspective of formal verification. Long employed in probability theory and theoretical computer science, these proofs construct couplings between the output distributions of two probabilistic processes. Couplings can imply various probabilistic relational properties, guarantees that compare two runs of a probabilistic computation. To give a formal account of this clean proof technique, we first show that proofs in the program logic pRHL (probabilistic Relational Hoare Logic) describe couplings. We formalize couplings that establish various probabilistic properties, including distribution equivalence, convergence, and stochastic domination. Then we deepen the connection between couplings and pRHL by giving a proofs-as-programs interpretation: a coupling proof encodes a probabilistic product program, whose properties imply relational properties of the original two programs. We design the logic xpRHL (product pRHL) to build the product program, with extensions to model more advanced constructions including shift coupling and path coupling. We then develop an approximate version of probabilistic coupling, based on approximate liftings. It is known that the existence of an approximate lifting implies differential privacy, a relational notion of statistical privacy. We propose a corresponding proof technique---proof by approximate coupling---inspired by the logic apRHL, a version of pRHL for building approximate liftings. Drawing on ideas from existing privacy proofs, we extend apRHL with novel proof rules for constructing new approximate couplings. We give approximate coupling proofs of privacy for the Report-noisy-max and Sparse Vector mechanisms, well-known algorithms from the privacy literature with notoriously subtle privacy proofs, and produce the first formalized proof of privacy for these algorithms in apRHL. Finally, we enrich the theory of approximate couplings with several more sophisticated constructions: a principle for showing accuracy-dependent privacy, a generalization of the advanced composition theorem from differential privacy, and an optimal approximate coupling relating two subsets of samples. We also show equivalences between approximate couplings and other existing definitions. These ingredients support the first formalized proof of privacy for the Between Thresholds mechanism, an extension of the Sparse Vector mechanism.},
  langid = {english},
  school = {University of Pennsylvania},
  file = {C:\Users\skyli\Zotero\storage\IISA6V9J\Hsu - 2017 - Probabilistic Couplings For Probabilistic Reasonin.pdf}
}

@inproceedings{jagielskiAuditingDifferentiallyPrivate2020,
  title = {Auditing {{Differentially Private Machine Learning}}: {{How Private}} Is {{Private SGD}}?},
  shorttitle = {Auditing {{Differentially Private Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jagielski, Matthew and Ullman, Jonathan and Oprea, Alina},
  year = {2020},
  volume = {33},
  pages = {22205--22216},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-20},
  abstract = {We investigate whether Differentially Private SGD offers better privacy in practice than what is guaranteed by its state-of-the-art analysis. We do so via novel data poisoning attacks, which we show correspond to realistic privacy attacks. While previous work (Ma et al., arXiv 2019) proposed this connection between differential privacy and data poisoning as a defense against data poisoning, our use as a tool for understanding the privacy of a specific mechanism is new. More generally, our work takes a quantitative, empirical approach to understanding the privacy afforded by specific implementations of differentially private algorithms that we believe has the potential to complement and influence analytical work on differential privacy.},
  file = {C:\Users\skyli\Zotero\storage\XH5VSX3B\Jagielski et al. - 2020 - Auditing Differentially Private Machine Learning .pdf}
}

@misc{LearningPrivacyScalea,
  title = {Learning with {{Privacy}} at {{Scale}}},
  journal = {Apple Machine Learning Research},
  urldate = {2024-03-19},
  abstract = {Understanding how people use their devices often helps in improving the user experience. However, accessing the data that provides such{\dots}},
  howpublished = {https://machinelearning.apple.com/research/learning-with-privacy-at-scale},
  langid = {american},
  file = {C:\Users\skyli\Zotero\storage\WG9HP4LH\learning-with-privacy-at-scale.html}
}

@book{lindvallLecturesCouplingMethod2002,
  title = {Lectures on the {{Coupling Method}}},
  author = {Lindvall, Torgny},
  year = {2002},
  month = jan,
  publisher = {Courier Corporation},
  abstract = {An important tool in probability theory and its applications, the coupling method is primarily used in estimates of total variation distances. The method also works well in establishing inequalities, and it has proven highly successful in the study of Markov and renewal process asymptotics. This text represents a detailed, comprehensive examination of the method and its broad variety of applications. Readers progress from simple to advanced topics, with end-of-discussion notes that reinforce the preceding material. Topics include renewal theory, Markov chains, Poisson approximation, ergodicity, and Strassen\&\#39;s theorem. A practical and easy-to-use reference, this volume will accommodate the diverse needs of professionals in the fields of statistics, mathematics, and operational research, as well as those of teachers and students.},
  googlebooks = {GUwyU1ypd1wC},
  isbn = {978-0-486-42145-2},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General}
}

@article{liuModelCheckingDifferentially2023,
  title = {Model Checking Differentially Private Properties},
  author = {Liu, Depeng and Wang, Bow-Yaw and Fu, Chen and Zhang, Lijun},
  year = {2023},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {943},
  pages = {153--170},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2022.10.002},
  urldate = {2024-03-20},
  abstract = {With the explosion of digital data collected from social apps, privacy protection regulations have been issued by almost all countries. Differential privacy is proposed as a successful technique to make use of these data, without leaking personal private data at the same time. In this paper, we investigate logical reasoning for differential privacy properties and propose model checking algorithms. We introduce the branching time temporal logic dpCTL* to specify differentially private properties. Several mechanisms in differential privacy are formalized as Markov chains or Markov decision processes. In our framework, we show that subtle privacy conditions are specified by dpCTL*. In order to verify privacy properties automatically, model checking problems are investigated. We develop a model checking algorithm for Markov chains. Model checking dpCTL* properties on Markov decision processes however is shown to be undecidable. Therefore, we propose a sound algorithm to verify these properties in Markov decision processes. We implement our model checking algorithm into a tool, based on the PRISM language, as well as show the experimental results and a case study of stream processing in differential privacy under the essential fairness assumptions.},
  keywords = {Differential privacy,Model checking,Temporal logic},
  file = {C:\Users\skyli\Zotero\storage\QFQXE4TI\S0304397522005837.html}
}

@inproceedings{loknaGroupAttackAuditing2023,
  title = {Group and {{Attack}}: {{Auditing Differential Privacy}}},
  shorttitle = {Group and {{Attack}}},
  booktitle = {Proceedings of the 2023 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Lokna, Johan and Paradis, Anouk and Dimitrov, Dimitar I. and Vechev, Martin},
  year = {2023},
  month = nov,
  series = {{{CCS}} '23},
  pages = {1905--1918},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3576915.3616607},
  urldate = {2024-03-19},
  abstract = {({$\varepsilon$}, {$\delta$}) differential privacy has seen increased adoption recently, especially in private machine learning applications. While this privacy definition allows provably limiting the amount of information leaked by an algorithm, practical implementations of differentially private algorithms often contain subtle vulnerabilities. This motivates the need for effective tools that can audit ({$\varepsilon$}, {$\delta$}) differential privacy algorithms before deploying them in the real world. However, existing state-of-the-art-tools for auditing ({$\varepsilon$}, {$\delta$}) differential privacy directly extend the tools for {$\varepsilon$}-differential privacy by fixing either {$\varepsilon$} or {$\delta$} in the violation search, inherently restricting their ability to efficiently discover violations of ({$\varepsilon$}, {$\delta$}) differential privacy. We present a novel method to efficiently discover ({$\varepsilon$}, {$\delta$}) differential privacy violations based on the key insight that many ({$\varepsilon$}, {$\delta$}) pairs can be grouped as they result in the same algorithm. Crucially, our method is orthogonal to existing approaches and, when combined, results in a faster and more precise violation search. We implemented our approach in a tool called Delta-Siege and demonstrated its effectiveness by discovering vulnerabilities in most of the evaluated frameworks, several of which were previously unknown. Further, in 84\% of cases, Delta-Siege outperforms existing state-of-the-art auditing tools. Finally, we show how Delta-Siege outputs can be used to find the precise root cause of vulnerabilities, an option no other differential privacy testing tool currently offers.},
  isbn = {9798400700507},
  keywords = {auditing,differential privacy},
  file = {C:\Users\skyli\Zotero\storage\65CXMP25\Lokna et al. - 2023 - Group and Attack Auditing Differential Privacy.pdf}
}

@misc{luGeneralFrameworkAuditing2022,
  title = {A {{General Framework}} for {{Auditing Differentially Private Machine Learning}}},
  author = {Lu, Fred and Munoz, Joseph and Fuchs, Maya and LeBlond, Tyler and {Zaresky-Williams}, Elliott and Raff, Edward and Ferraro, Francis and Testa, Brian},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08643},
  eprint = {2210.08643},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.08643},
  urldate = {2024-03-20},
  abstract = {We present a framework to statistically audit the privacy guarantee conferred by a differentially private machine learner in practice. While previous works have taken steps toward evaluating privacy loss through poisoning attacks or membership inference, they have been tailored to specific models or have demonstrated low statistical power. Our work develops a general methodology to empirically evaluate the privacy of differentially private machine learning implementations, combining improved privacy search and verification methods with a toolkit of influence-based poisoning attacks. We demonstrate significantly improved auditing power over previous approaches on a variety of models including logistic regression, Naive Bayes, and random forest. Our method can be used to detect privacy violations due to implementation errors or misuse. When violations are not present, it can aid in understanding the amount of information that can be leaked from a given dataset, algorithm, and privacy specification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\skyli\\Zotero\\storage\\M5DWHBXI\\Lu et al. - 2022 - A General Framework for Auditing Differentially Pr.pdf;C\:\\Users\\skyli\\Zotero\\storage\\NPKASSGL\\2210.html}
}

@misc{lyuUnderstandingSparseVector2016a,
  title = {Understanding the {{Sparse Vector Technique}} for {{Differential Privacy}}},
  author = {Lyu, Min and Su, Dong and Li, Ninghui},
  year = {2016},
  month = sep,
  number = {arXiv:1603.01699},
  eprint = {1603.01699},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.01699},
  urldate = {2023-10-18},
  abstract = {The Sparse Vector Technique (SVT) is a fundamental technique for satisfying differential privacy and has the unique quality that one can output some query answers without apparently paying any privacy cost. SVT has been used in both the interactive setting, where one tries to answer a sequence of queries that are not known ahead of the time, and in the non-interactive setting, where all queries are known. Because of the potential savings on privacy budget, many variants for SVT have been proposed and employed in privacy-preserving data mining and publishing. However, most variants of SVT are actually not private. In this paper, we analyze these errors and identify the misunderstandings that likely contribute to them. We also propose a new version of SVT that provides better utility, and introduce an effective technique to improve the performance of SVT. These enhancements can be applied to improve utility in the interactive setting. Through both analytical and experimental comparisons, we show that, in the non-interactive setting (but not the interactive setting), the SVT technique is unnecessary, as it can be replaced by the Exponential Mechanism (EM) with better accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\skyli\\Zotero\\storage\\6D5T4AZE\\Lyu et al. - 2016 - Understanding the Sparse Vector Technique for Diff.pdf;C\:\\Users\\skyli\\Zotero\\storage\\NKSB3EPQ\\1603.html}
}

@incollection{nagyUnionFreenessDeterministicUnionFreeness2019,
  title = {Union-{{Freeness}}, {{Deterministic Union-Freeness}} and {{Union-Complexity}}},
  booktitle = {Descriptional {{Complexity}} of {{Formal Systems}}},
  author = {Nagy, Benedek},
  editor = {Hospod{\'a}r, Michal and Jir{\'a}skov{\'a}, Galina and Konstantinidis, Stavros},
  year = {2019},
  volume = {11612},
  pages = {46--56},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-23247-4_3},
  urldate = {2024-02-16},
  abstract = {Union-free expressions are regular expressions without using the union operation. Consequently, union-free languages are described by regular expressions using only concatenation and Kleene star. The language class is also characterised by a special class of finite automata: 1CFPAs have exactly one cycle-free accepting path from each of their states. Obviously such an automaton has exactly one accepting state. The deterministic counterpart of such class of automata defines the deterministic union-free languages. A regular expression is in union (disjunctive) normal form if it is a finite union of union-free expressions. By manipulating regular expressions, each of them has equivalent expression in union normal form. By the minimum number of union-free expressions needed to describe a regular language, its union-complexity is defined. For any natural number n there are languages such that their union complexity is n. However, there is not known any simple algorithm to determine the union-complexity of any language. Regarding the deterministic union-free languages, there are regular languages such that they cannot be written as a union of finitely many deterministic union-free languages.},
  isbn = {978-3-030-23246-7 978-3-030-23247-4},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\PH644BSL\Nagy - 2019 - Union-Freeness, Deterministic Union-Freeness and U.pdf}
}

@article{nagyUnionfreeRegularLanguages2006,
  title = {Union-Free Regular Languages and 1-Cycle-Free-Path-Automata},
  author = {Nagy, Benedek},
  year = {2006},
  month = jan,
  journal = {Publicationes Mathematicae Debrecen},
  volume = {68},
  number = {1-2},
  pages = {183--197},
  issn = {00333883},
  doi = {10.5486/PMD.2006.3303},
  urldate = {2024-02-16},
  abstract = {In this paper, we analyze a subclass of the regular languages, namely the union-free regular languages. These languages can be given by regular expressions without the operation union. In a union-free language the words look like each other, each word contains the so-called ``backbone'' word of the language in scattered way. The family of special type of finite automata is investigated to recognize these languages. These automata are the 1-cycle-free-path-automata. In these class from each state there is exactly one cycle-free path going to the final state. We also have result about regular expressions with union describing union-free languages.},
  langid = {english},
  file = {C:\Users\skyli\Zotero\storage\FW3TXWSI\Nagy - 2006 - Union-free regular languages and 1-cycle-free-path.pdf}
}

@inproceedings{niuDPOptIdentifyHigh2022,
  title = {{{DP-Opt}}: {{Identify High Differential Privacy Violation}} by~{{Optimization}}},
  shorttitle = {{{DP-Opt}}},
  booktitle = {Wireless {{Algorithms}}, {{Systems}}, and {{Applications}}},
  author = {Niu, Ben and Zhou, Zejun and Chen, Yahong and Cao, Jin and Li, Fenghua},
  editor = {Wang, Lei and Segal, Michael and Chen, Jenhui and Qiu, Tie},
  year = {2022},
  pages = {406--416},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19214-2_34},
  abstract = {Differential privacy has become a golden standard for designing privacy-preserving randomized algorithms. However, such algorithms are subtle to design, as many of them are found to have incorrect privacy claim. To help identify this problem, one approach is designing disprovers to search for counterexamples that demonstrate high violation of claimed privacy level. In this paper, we present DP-Opt(mizer), a disprover that tries to search for counterexamples whose lower bounds on differential privacy exceed the claimed level of privacy guaranteed by the algorithm. We leverage the insights of counterexample construction proposed by the latest work, meanwhile resolve their limitations. We transform the search task into an improved optimization objective which takes into account the empirical error, then solve it with various off-the-shelf optimizers. An evaluation on a variety of both correct and incorrect algorithms illustrates that DP-Opt almost always produces stronger guarantees than the latest work up to a factor of 9.42, with runtime reduced by an average of \$\$19.2{\textbackslash}\%\$\$19.2\%.},
  isbn = {978-3-031-19214-2},
  langid = {english},
  keywords = {Differential privacy,Disprover,Lower bounds},
  file = {C:\Users\skyli\Zotero\storage\YRVH2MZJ\Niu et al. - 2022 - DP-Opt Identify High Differential Privacy Violati.pdf}
}

@misc{PrivacyFeaturesa,
  title = {Privacy - {{Features}}},
  journal = {Apple},
  urldate = {2024-03-19},
  abstract = {Apple is constantly improving the built-in technologies designed to keep your personal information safe.},
  howpublished = {https://www.apple.com/privacy/features/},
  langid = {american},
  file = {C:\Users\skyli\Zotero\storage\JPD8GI8P\features.html}
}

@article{reedDistanceMakesTypes2010,
  title = {Distance Makes the Types Grow Stronger: A Calculus for Differential Privacy},
  shorttitle = {Distance Makes the Types Grow Stronger},
  author = {Reed, Jason and Pierce, Benjamin C.},
  year = {2010},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {45},
  number = {9},
  pages = {157--168},
  issn = {0362-1340},
  doi = {10.1145/1932681.1863568},
  urldate = {2024-03-19},
  abstract = {We want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. Differential privacy offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacy-safe query programs in a flexible and compositional way. The key novelty is the way our type system captures function sensitivity, a measure of how much a function can magnify the distance between similar inputs: well-typed programs not only can't go wrong, they can't go too far on nearby inputs. Moreover, by introducing a monad for random computations, we can show that the established definition of differential privacy falls out naturally as a special case of this soundness principle. We develop examples including known differentially private algorithms, privacy-aware variants of standard functional programming idioms, and compositionality principles for differential privacy.},
  keywords = {differential privacy,type systems}
}

@misc{steinkePrivacyAuditingOne2023,
  title = {Privacy {{Auditing}} with {{One}} (1) {{Training Run}}},
  author = {Steinke, Thomas and Nasr, Milad and Jagielski, Matthew},
  year = {2023},
  month = may,
  number = {arXiv:2305.08846},
  eprint = {2305.08846},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.08846},
  urldate = {2024-03-19},
  abstract = {We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {C\:\\Users\\skyli\\Zotero\\storage\\XDK288PU\\Steinke et al. - 2023 - Privacy Auditing with One (1) Training Run.pdf;C\:\\Users\\skyli\\Zotero\\storage\\KYQHMA9T\\2305.html}
}

@misc{tangPrivacyLossApple2017,
  title = {Privacy {{Loss}} in {{Apple}}'s {{Implementation}} of {{Differential Privacy}} on {{MacOS}} 10.12},
  author = {Tang, Jun and Korolova, Aleksandra and Bai, Xiaolong and Wang, Xueqiang and Wang, Xiaofeng},
  year = {2017},
  month = sep,
  number = {arXiv:1709.02753},
  eprint = {1709.02753},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.02753},
  urldate = {2023-12-10},
  abstract = {In June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy. In this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple's set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple's servers. We find that although Apple's deployment ensures that the (differential) privacy loss per each datum submitted to its servers is \$1\$ or \$2\$, the overall privacy loss permitted by the system is significantly higher, as high as \$16\$ per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications. We advocate that in order to claim the full benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the privacy loss permitted.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\skyli\\Zotero\\storage\\WJXN99ZN\\Tang et al. - 2017 - Privacy Loss in Apple's Implementation of Differen.pdf;C\:\\Users\\skyli\\Zotero\\storage\\EIQST3FL\\1709.html}
}

@article{toroContextualLinearTypes2023,
  title = {Contextual {{Linear Types}} for {{Differential Privacy}}},
  author = {Toro, Mat{\'i}as and Darais, David and Abuah, Chike and Near, Joseph P. and {\'A}rquez, Dami{\'a}n and Olmedo, Federico and Tanter, {\'E}ric},
  year = {2023},
  month = may,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {45},
  number = {2},
  pages = {8:1--8:69},
  issn = {0164-0925},
  doi = {10.1145/3589207},
  urldate = {2024-03-19},
  abstract = {Language support for differentially private programming is both crucial and delicate. While elaborate program logics can be very expressive, type-system-based approaches using linear types tend to be more lightweight and amenable to automatic checking and inference, and in particular in the presence of higher-order programming. Since the seminal design of Fuzz, which is restricted to {$\epsilon$}-differential privacy in its original design, significant progress has been made to support more advanced variants of differential privacy, like ({$\epsilon$}, {$\delta$})-differential privacy. However, supporting these advanced privacy variants while also supporting higher-order programming in full has proven to be challenging. We present Jazz, a language and type system that uses linear types and latent contextual effects to support both advanced variants of differential privacy and higher-order programming. Latent contextual effects allow delaying the payment of effects for connectives such as products, sums, and functions, yielding advantages in terms of precision of the analysis and annotation burden upon elimination, as well as modularity. We formalize the core of Jazz, prove it sound for privacy via a logical relation for metric preservation, and illustrate its expressive power through a number of case studies drawn from the recent differential privacy literature.},
  keywords = {differential privacy,Type systems},
  file = {C:\Users\skyli\Zotero\storage\BBBYCGMD\Toro et al. - 2023 - Contextual Linear Types for Differential Privacy.pdf}
}

@article{tschantzFormalVerificationDifferential2011,
  title = {Formal {{Verification}} of {{Differential Privacy}} for {{Interactive Systems}} ({{Extended Abstract}})},
  author = {Tschantz, Michael Carl and Kaynar, Dilsun and Datta, Anupam},
  year = {2011},
  month = sep,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {Twenty-Seventh {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXVII}})},
  volume = {276},
  pages = {61--79},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2011.09.015},
  urldate = {2024-03-20},
  abstract = {Differential privacy is a promising approach to privacy preserving data analysis with a well-developed theory for functions. Despite recent work on implementing systems that aim to provide differential privacy, the problem of formally verifying that these systems have differential privacy has not been adequately addressed. We develop a formal probabilistic automaton model of differential privacy for systems by adapting prior work on differential privacy for functions. We present the first sound verification technique for proving differential privacy of interactive systems. The technique is based on a form of probabilistic bisimulation relation. The novelty lies in the way we track quantitative privacy leakage bounds using a relation family instead of a single relation. We illustrate the proof technique on a representative automaton motivated by PINQ, an implemented system that is intended to provide differential privacy. Surprisingly, our analysis yields a privacy leakage bound of (2t{$\epsilon$}) rather than (t{$\epsilon$}) when {$\epsilon$}-differentially private functions are called t times. The extra leakage arises from accounting for bounded memory constraints of real computers.},
  keywords = {Bisimulation,Differential Privacy,Formal Methods,Privacy,Verification},
  file = {C:\Users\skyli\Zotero\storage\STLGH993\S157106611100106X.html}
}

@inproceedings{wangCheckDPAutomatedIntegrated2020,
  title = {{{CheckDP}}: {{An Automated}} and {{Integrated Approach}} for {{Proving Differential Privacy}} or {{Finding Precise Counterexamples}}},
  shorttitle = {{{CheckDP}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Wang, Yuxin and Ding, Zeyu and Kifer, Daniel and Zhang, Danfeng},
  year = {2020},
  month = nov,
  series = {{{CCS}} '20},
  pages = {919--938},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3372297.3417282},
  urldate = {2024-03-19},
  abstract = {We propose CheckDP, an automated and integrated approach for proving or disproving claims that a mechanism is differentially private. CheckDP can find counterexamples for mechanisms with subtle bugs for which prior counterexample generators have failed. Furthermore, it was able to automatically generate proofs for correct mechanisms for which no formal verification was reported before. CheckDP is built on static program analysis, allowing it to be more efficient and precise in catching infrequent events than sampling based counterexample generators (which run mechanisms hundreds of thousands of times to estimate their output distribution). Moreover, its sound approach also allows automatic verification of correct mechanisms. When evaluated on standard benchmarks and newer privacy mechanisms, CheckDP generates proofs (for correct mechanisms) and counterexamples (for incorrect mechanisms) within 70 seconds without any false positives or false negatives.},
  isbn = {978-1-4503-7089-9},
  keywords = {counterexample detection,differential privacy,formal verification},
  file = {C:\Users\skyli\Zotero\storage\VDUULBFQ\Wang et al. - 2020 - CheckDP An Automated and Integrated Approach for .pdf}
}

@inproceedings{wangProvingDifferentialPrivacy2019,
  title = {Proving Differential Privacy with Shadow Execution},
  booktitle = {Proceedings of the 40th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Wang, Yuxin and Ding, Zeyu and Wang, Guanhong and Kifer, Daniel and Zhang, Danfeng},
  year = {2019},
  month = jun,
  series = {{{PLDI}} 2019},
  pages = {655--669},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3314221.3314619},
  urldate = {2024-03-19},
  abstract = {Recent work on formal verification of differential privacy shows a trend toward usability and expressiveness -- generating a correctness proof of sophisticated algorithm while minimizing the annotation burden on programmers. Sometimes, combining those two requires substantial changes to program logics: one recent paper is able to verify Report Noisy Max automatically, but it involves a complex verification system using customized program logics and verifiers. In this paper, we propose a new proof technique, called shadow execution, and embed it into a language called ShadowDP. ShadowDP uses shadow execution to generate proofs of differential privacy with very few programmer annotations and without relying on customized logics and verifiers. In addition to verifying Report Noisy Max, we show that it can verify a new variant of Sparse Vector that reports the gap between some noisy query answers and the noisy threshold. Moreover, ShadowDP reduces the complexity of verification: for all of the algorithms we have evaluated, type checking and verification in total takes at most 3 seconds, while prior work takes minutes on the same algorithms.},
  isbn = {978-1-4503-6712-7},
  keywords = {dependent types,Differential privacy},
  file = {C:\Users\skyli\Zotero\storage\LS9G3RNA\Wang et al. - 2019 - Proving differential privacy with shadow execution.pdf}
}

@inproceedings{zhangLightDPAutomatingDifferential2017,
  title = {{{LightDP}}: Towards Automating Differential Privacy Proofs},
  shorttitle = {{{LightDP}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Zhang, Danfeng and Kifer, Daniel},
  year = {2017},
  month = jan,
  series = {{{POPL}} '17},
  pages = {888--901},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3009837.3009884},
  urldate = {2024-03-19},
  abstract = {The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal verification tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but come with a steep learning curve and significant annotation burden on the programmers, while existing programming platforms lack expressive power for some sophisticated algorithms. In this paper, we present LightDP, a simple imperative language that strikes a better balance between expressive power and usability. The core of LightDP is a novel relational type system that separates relational reasoning from privacy budget calculations. With dependent types, the type system is powerful enough to verify sophisticated algorithms where the composition theorem falls short. In addition, the inference engine of LightDP infers most of the proof details, and even searches for the proof with minimal privacy cost when multiple proofs exist. We show that LightDP verifies sophisticated algorithms with little manual effort.},
  isbn = {978-1-4503-4660-3},
  keywords = {dependent types,Differential privacy,type inference},
  file = {C:\Users\skyli\Zotero\storage\J8TL4TQ9\Zhang and Kifer - 2017 - LightDP towards automating differential privacy p.pdf}
}

@article{zhangTestingDifferentialPrivacy2020,
  title = {Testing Differential Privacy with Dual Interpreters},
  author = {Zhang, Hengchu and Roth, Edo and Haeberlen, Andreas and Pierce, Benjamin C. and Roth, Aaron},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {OOPSLA},
  pages = {165:1--165:26},
  doi = {10.1145/3428233},
  urldate = {2024-03-20},
  abstract = {Applying differential privacy at scale requires convenient ways to check that programs computing with sensitive data appropriately preserve privacy. We propose here a fully automated framework for testing differential privacy, adapting a well-known ``pointwise'' technique from informal proofs of differential privacy. Our framework, called DPCheck, requires no programmer annotations, handles all previously verified or tested algorithms, and is the first fully automated framework to distinguish correct and buggy implementations of PrivTree, a probabilistically terminating algorithm that has not previously been mechanically checked. We analyze the probability of DPCheck mistakenly accepting a non-private program and prove that, theoretically, the probability of false acceptance can be made exponentially small by suitable choice of test size. We demonstrate DPCheck's utility empirically by implementing all benchmark algorithms from prior work on mechanical verification of differential privacy, plus several others and their incorrect variants, and show DPCheck accepts the correct implementations and rejects the incorrect variants. We also demonstrate how DPCheck can be deployed in a practical workflow to test differentially privacy for the 2020 US Census Disclosure Avoidance System (DAS).},
  keywords = {Differential privacy,symbolic execution,testing},
  file = {C:\Users\skyli\Zotero\storage\NXQ8TEUS\Zhang et al. - 2020 - Testing differential privacy with dual interpreter.pdf}
}
